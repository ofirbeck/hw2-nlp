{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e18785ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "533b92fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec_model(sentences, sg, window_size):    \n",
    "    model = Word2Vec(sentences, window=window_size, min_count=1, sg=sg)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d939dec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences: [['the', 'bank', 'is', 'located', 'near', 'the', 'river', '.'], ['the', 'bank', 'approved', 'my', 'loan', 'application', '.'], ['he', 'rose', 'from', 'his', 'chair', 'to', 'close', 'the', 'window', '.'], ['the', 'rose', 'bloomed', 'beautifully', 'in', 'the', 'garden', '.'], ['the', 'lead', 'actor', 'delivered', 'a', 'stunning', 'performance', '.'], ['exposure', 'to', 'lead', 'is', 'harmful', 'to', 'health', '.'], ['she', 'is', 'reading', 'a', 'book', 'in', 'the', 'library', '.'], ['the', 'book', 'mentioned', 'a', 'fascinating', 'historical', 'event', '.'], ['i', 'need', 'to', 'file', 'a', 'report', 'for', 'my', 'manager', '.'], ['he', 'lost', 'the', 'file', 'containing', 'important', 'documents', '.']]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"The bank is located near the river.\", \"The bank approved my loan application.\", \"He rose from his chair to close the window.\", \"The rose bloomed beautifully in the garden.\", \"The lead actor delivered a stunning performance.\", \"Exposure to lead is harmful to health.\", \"She is reading a book in the library.\", \"The book mentioned a fascinating historical event.\", \"I need to file a report for my manager.\", \"He lost the file containing important documents.\"]\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "print(\"Tokenized sentences:\", tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "71a70d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Model: [('containing', 0.1821729987859726), ('a', 0.1726524829864502), ('my', 0.16703678667545319)]\n"
     ]
    }
   ],
   "source": [
    "# try both skip-gram and CBOW\n",
    "cbow_model = train_word2vec_model(tokenized_sentences, sg=0, window_size=3)\n",
    "sg_model = train_word2vec_model(tokenized_sentences, sg=1, window_size=3)\n",
    "print(\"CBOW Model:\", cbow_model.wv.most_similar('bank', topn=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e913659",
   "metadata": {},
   "source": [
    "## d. Differences between the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_similarity(word, cbow_model, sg_model):\n",
    "    cbow_vector = cbow_model.wv[word]\n",
    "    sg_vector = sg_model.wv[word]\n",
    "    # find a word which has similar embedding in both models\n",
    "    cousine_similarity = cosine_similarity(cbow_vector.reshape(1, -1), sg_vector.reshape(1, -1))\n",
    "    #print(f\"Cosine similarity between CBOW and Skip-gram vectors for '{word}':\", cousine_similarity[0][0])\n",
    "    return cousine_similarity[0][0]\n",
    "\n",
    "def get_most_similar_embeddings(cbow_model, sg_model):\n",
    "    similarity_per_token = {}\n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence:\n",
    "            similarity = caculate_similarity(token, cbow_model, sg_model)\n",
    "            similarity_per_token[token] = similarity\n",
    "    #print(\"Similarity per token:\", similarity_per_token)\n",
    "    print(\"The most similar embeddings between CBOW and Skip-gram models are: \", sorted(similarity_per_token.items(), key=lambda x: x[1], reverse=True)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "54f39b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar embeddings between CBOW and Skip-gram models are:  [('library', 1.0000001), ('application', 1.0), ('bloomed', 1.0), ('in', 1.0), ('performance', 1.0)]\n",
      "The most similar embeddings between CBOW and Skip-gram models are:  [('near', 1.0000001), ('i', 1.0000001), ('bank', 1.0), ('application', 1.0), ('in', 1.0)]\n",
      "The most similar embeddings between CBOW and Skip-gram models are:  [('i', 1.0000001), ('application', 1.0), ('located', 0.99999994), ('approved', 0.99999994), ('loan', 0.99999994)]\n"
     ]
    }
   ],
   "source": [
    "# the most similar embeddings between CBOW and Skip-gram models for window size 3\n",
    "get_most_similar_embeddings(cbow_model, sg_model)\n",
    "# for window size 5\n",
    "cbow_model_5 = train_word2vec_model(tokenized_sentences, sg=0, window_size=5)\n",
    "sg_model_5 = train_word2vec_model(tokenized_sentences, sg=1, window_size=5)\n",
    "get_most_similar_embeddings(cbow_model_5, sg_model_5)\n",
    "\n",
    "# for window size 7\n",
    "cbow_model_7 = train_word2vec_model(tokenized_sentences, sg=0, window_size=7)\n",
    "sg_model_7 = train_word2vec_model(tokenized_sentences, sg=1, window_size=7)\n",
    "get_most_similar_embeddings(cbow_model_7, sg_model_7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
